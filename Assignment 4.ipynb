{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e815b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fba64660",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dg=ImageDataGenerator(vertical_flip=False,horizontal_flip=True,zoom_range=0.25,rescale=1/255,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e6014db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11976 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train=train_dg.flow_from_directory(directory=\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\AI COURSE\\\\shapes\",batch_size=32,class_mode=\"categorical\"\n",
    "                            ,target_size=(64,64),subset=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fb581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2994 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "X_test=train_dg.flow_from_directory(directory=\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\AI COURSE\\\\shapes\",batch_size=32,class_mode=\"categorical\"\n",
    "                            ,target_size=(64,64),subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "439c89ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'circle': 0, 'square': 1, 'star': 2, 'triangle': 3}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "254b2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Convolution2D,Dense,Flatten,MaxPooling2D\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "36028fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6abe78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32,(32,32),activation='relu',input_shape=(64,64,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eb33e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "af98b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(150,kernel_initializer=RandomNormal,activation=\"relu\"))\n",
    "model.add(Dense(75,kernel_initializer=RandomNormal,activation=\"relu\"))\n",
    "model.add(Dense(4,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "72253b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=CategoricalCrossentropy(),\n",
    "             optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9fa4d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 116s 310ms/step - loss: 0.1582 - accuracy: 0.9375 - val_loss: 0.0046 - val_accuracy: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c918521198>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(X_train,steps_per_epoch=len(X_train),epochs=1,validation_data=X_test,validation_steps=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "60a4f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"shapes.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6c0d8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={0:'circle', 1:'square', 2:'star', 3:'triangle'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b9b75051",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img(\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\AI COURSE\\\\shapes\\\\triangle\\\\680.png\",target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2abbbec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABo0lEQVR4nO2ZPY6DMBCFPdZWoAhBxTGQLISo4QZwFG7DhWig4AD0VDQISqdglY20u0n8OyD5q1AUXt7LaGwzAOecXBmKbUAVFwAbFwAbFwAbFwAbFwAbFwCbywf4wjbwDQAcF6LHe/wKRFH0cC8BZgDGGAAsy6IighOgrus4jodh0KDF7dI0zWs/XdcJCdprYs/z9n1/+7VxHLMsE9A19E8/kySJOT9mK6CyvHyIqSYuy9KCe2JiI5vnOY5j7bL/oTPAtm2+72sU/ARtASilHGPIp9oDaZoCAACguCcqFbDTo2+RrMBJ3BOJCpzH+oFYBdq2NeRDGpnmM1oEUT8yPXAcQiRuNIH8MnqSDEr7wBlKoeEwd8TI81xdSgLNO+jtdlvXVUVB1I/+IwDnnFJ7raX/eeA4FxVFoV35758z2oUSOwZ+BZ6xsEzZmAsZzWBpsGWuFFYncyZiIIwWOedVVelSQ3sUJIQwxn6PR8+1Cr2m7/tpmsIwfHwSBIGoCGYFfkzIvt0gJwmgAv4bGkVcAGxcAGxcAGxcAGxcAGxcAGwuH+AOjYu892N8TmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x2C926C227F0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c5bdd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "im=image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ffd57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be8dede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'triangle'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[np.where(model.predict(np.expand_dims(im,axis=0)))[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3e5a5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "57aa7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "video=cv2.VideoCapture(0)\n",
    "while 1:\n",
    "    sucess,frame=video.read()\n",
    "    cv2.imwrite('img.jpg',frame)\n",
    "    img=image.load_img('img.jpg',target_size=(64,64))\n",
    "    X=image.img_to_array(img)\n",
    "    x=np.expand_dims(img,axis=0)\n",
    "    pred=dic[np.where(model.predict(x))[1][0]]\n",
    "    cv2.putText(frame,pred,(100,100),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),4)\n",
    "    cv2.imshow('image',frame)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
